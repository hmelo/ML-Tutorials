{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## 1. Background\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of deep neural network particularly useful for processing images. They take inspiration from biological processing in the visual cortex in animals. In particular they exploit the idea of a receptive field where neurons respond to specific regions of the visual field. The idea is to cover the visual field with some overlap between receptive fields.\n",
    "\n",
    "Like other neural networks, CNNs consist of an input layer, which usually take an image as the input, and a number of specialized hidden layers to process features, and finally an output layer which usually classifies the image into a class.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png\" height=\"350\" width=\"650\"/>\n",
    "\n",
    "\n",
    "\n",
    "Hidden layers include convolutional, pooling, fully connceted layers.\n",
    "\n",
    "<b>Convolutional layers</b> effectively apply a kernel over the data which allows for sparse interactions, parameter sharing, and equivariant representations. <b>Pooling layers</b> replaces the output of the net at a certain location with asummary statistic of the nearby outputs such as max pooling where it it outputs the highest value within the repective field, this allows for a certain degree of invariance in the sense that if you were to move the image a little the network wouldn't be affected too much.\n",
    "\n",
    "For a more detailed description you can read [chapter 9](http://www.deeplearningbook.org/contents/convnets.html) of the Deep Learning book by Goodfellow et al. also the following [video](https://www.youtube.com/watch?v=bEUX_56Lojc&index=13&list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu) by Nando de Freitas.\n",
    "\n",
    "In the following examples I use TensorFlow to demonstrate a simple convnet on the notMNIST dataset.\n",
    "\n",
    "<ul><li>2.1 Data Preparation</li>\n",
    "<li>2.2 Simple CNN with 2 convolutional layers</li>\n",
    "<li>2.3 CNN with pooling</li>\n",
    "<li>2.4 CNN with dropout regulatization</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example CNN  \n",
    "\n",
    "### 2.1 Data Preparation\n",
    "First we need to optain the dataset and get it ready for use in the CNN. \n",
    "We'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given these sizes, it should be possible to train models quickly on any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large.tar.gz\n",
      "Found and verified /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '/Users/Home/Documents/Machine Learning/ML Tutorials' # Change to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the dataset from the compressed .tar.gz file. This should give you a set of directories, labelled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large already present - Skipping extraction of /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large.tar.gz.\n",
      "['/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/B', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/C', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/D', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/E', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/F', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/G', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/H', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/I', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/J']\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small already present - Skipping extraction of /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small.tar.gz.\n",
      "['/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/A', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/B', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/C', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/D', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/E', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/F', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/G', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/H', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/I', '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/B\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/C\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/D\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/E\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/F\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/G\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/H\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/I\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_large/J\n",
      "Full dataset tensor: (529114, 28, 28)\n",
      "Mean: -0.0816596\n",
      "Labels: (529114,)\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/A\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/B\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/C\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/D\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/E\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/F\n",
      "Could not read: /Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file '/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/G\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/H\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/I\n",
      "/Users/Home/Documents/Machine Learning/ML Tutorials/notMNIST_small/J\n",
      "Full dataset tensor: (18724, 28, 28)\n",
      "Mean: -0.0746362\n",
      "Labels: (18724,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load(data_folders, min_num_images, max_num_images):\n",
    "  dataset = np.ndarray(\n",
    "    shape=(max_num_images, image_size, image_size), dtype=np.float32)\n",
    "  labels = np.ndarray(shape=(max_num_images), dtype=np.int32)\n",
    "  label_index = 0\n",
    "  image_index = 0\n",
    "  for folder in data_folders:\n",
    "    print(folder)\n",
    "    for image in os.listdir(folder):\n",
    "      if image_index >= max_num_images:\n",
    "        raise Exception('More images than expected: %d >= %d' % (\n",
    "          image_index, max_num_images))\n",
    "      image_file = os.path.join(folder, image)\n",
    "      try:\n",
    "        image_data = (ndimage.imread(image_file).astype(float) -\n",
    "                      pixel_depth / 2) / pixel_depth\n",
    "        if image_data.shape != (image_size, image_size):\n",
    "          raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "        dataset[image_index, :, :] = image_data\n",
    "        labels[image_index] = label_index\n",
    "        image_index += 1\n",
    "      except IOError as e:\n",
    "        print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    label_index += 1\n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  labels = labels[0:num_images]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' % (\n",
    "        num_images, min_num_images))\n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  # uncomment this if enough memory ...\n",
    "  #print('Standard deviation:', np.std(dataset))\n",
    "  print('Labels:', labels.shape)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = load(train_folders, 450000, 550000)\n",
    "test_dataset, test_labels = load(test_folders, 18000, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set 529114\n",
      "size of label set 529114\n",
      "size of image row 28\n",
      "size of image col 28\n",
      "label values [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print('size of training set ' + str(len(train_dataset)))\n",
    "print('size of label set ' + str(len(train_labels)))\n",
    "print('size of image row ' + str(len(train_dataset[0])))\n",
    "print('size of image col ' + str(len(train_dataset[0][0])))\n",
    "print('label values ' + str(np.unique(train_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letter(i):\n",
    "    return 'abcdefghij'[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11ccf7080>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFAJJREFUeJzt3X2QVNWZBvDnGWYAQUS+MrJARCKKrFFMTQANRrNkjZCN\nqNl1g0owawnZoOJHNJaaSNis5cZVgsawolJidAlxlYAuagmb1fiFDgYFxQRxQUC+FBT8AIaZd//o\nS6rFue9t5nb37Znz/Kqo6em3T/eZZp653X3uOYdmBhEJT1XWHRCRbCj8IoFS+EUCpfCLBErhFwmU\nwi8SKIU/QCSnkLw/635IthT+NorkuSTrSX5IciPJx0iOyLpfUjkU/jaI5BUAfgHgRgC1AD4P4A4A\nZ5TgsaqLfZ9SHgp/G0OyK4CpACaZ2cNm9pGZNZjZo2Z2dd5N25O8j+ROkq+RrMu7j2tIro5qr5M8\nK692AclnSU4j+R6AKc30YSjJ50m+H73q+CXJ9iX8saUFFP6250QAHQHMS7jdGQB+A+BQAAsA/DKv\nthrAyQC6AvgpgPtJ9s6rDwPwFnKvKv61mftuBHA5gJ5Rf0YC+MGB/iBSWgp/29MDwLtmtjfhds+Y\n2UIzawTwawDH7yuY2YNm9o6ZNZnZXACrAAzNa/uOmd1uZnvN7JP979jMlprZC1F9DYA7AZyS9geT\n4lL42573APQs4L34przLHwPouK8Nye+SXBa9bH8fwLHIHcX3WefdMcmjSD5KchPJHch99tDTayPl\np/C3Pc8D2A3gzJY0Jnk4gLsAXAygh5kdCmAFAObdLGkq6AwAbwAYaGaHALh2v/ZSART+NsbMPgDw\nEwB3kDyTZCeSNSRHkfx5AXfRGblwbwUAkt9D7sh/ILoA2AHgQ5KDAPzzAbaXMlD42yAzuwXAFQCu\nRy7E65A7kv+ugLavA7gFuVcQmwF8EcCzB9iFHwI4F8BO5F5FzD3A9lIG1GIeImHSkV8kUAq/SKAU\nfpFAKfwigSrrpIz27GAd0bmcDxmGo2piS12qd7tNd76uv/9tyS58hD22u6BzKlKFn+TpAKYDaAfg\nbjO7ybt9R3TGMI5M85BtExP+r5JGZP6jb2zp5F5vuk3/cFxH/76TpO27FNUSW1zwbVv8Z59kO+Sm\niY4CMBjAWJKDW3p/IlJeaV7zDQXwppm9ZWZ7kJshNqY43RKRUksT/j749ASP9dF1n0JyQrSiTH0D\n/PefIlI+Jf+0x8xmmlmdmdXVoEOpH05ECpQm/BsA9Mv7vm90nYi0AmnC/xKAgSSPiJZo+g5yK8KI\nSCvQ4qE+M9tL8mIATyA31DfLzF4rWs/akpTDYVUd/eG4E7q/HVu7vucbbtujf+bPtu1//fNundXx\n5xgAgDXsceuSnVTj/Ga2EMDCIvVFRMpIp3eJBErhFwmUwi8SKIVfJFAKv0igFH6RQGmTxXJgwt9Y\na3TLn4z8olu/qPu02NoJN14dWwOA2y+7y63/7KXvufWD5r/o1lHVLr7W5P/cUlo68osESuEXCZTC\nLxIohV8kUAq/SKAUfpFAlXWvvkPY3UJcvZfV/oiq7d3r1rc9epRbP67nO7G19cM/dNuumj7crZ97\nir9H50tDnKE8wJ/OrJV9i26JLcYO21bQ0t068osESuEXCZTCLxIohV8kUAq/SKAUfpFAKfwigdKU\n3mLwpq0ieRy/6vhj3PqC4/xpt2N+fFVsrRv8pbcH3fiWWz9h1Fq3/ugl57v12tufi62lPf9B0tGR\nXyRQCr9IoBR+kUAp/CKBUvhFAqXwiwRK4RcJlObzF0Ha8eo1c49z62cMXO7WX/1S/P9h2r6tu+4k\ntz7pvEfc+oLBPdy6FNeBzOdPdZIPyTUAdgJoBLDXzOrS3J+IlE8xzvD7mpm9W4T7EZEy0nt+kUCl\nDb8BWERyKckJzd2A5ASS9STrG7A75cOJSLGkfdk/wsw2kPwcgCdJvmFmT+ffwMxmApgJ5D7wS/l4\nIlIkqY78ZrYh+roFwDwAQ4vRKREpvRaHn2Rnkl32XQZwGoAVxeqYiJRWmpf9tQDmMbcuezWA/zSz\nx4vSq0rkzNlPGitvN9hfd3/h8F+59bE/iZ+vDyTM2U/aHjxB/1+tdOtdxn3i1jdfGn+eQO1t8XP9\nAc33L7UWh9/M3gJwfBH7IiJlpKE+kUAp/CKBUvhFAqXwiwRK4RcJlJbuLhCr4mdJWpPfduXkrm79\n/vf9c6O6zfaX3/a2wbaGPX7TmvZuvXH7drd+w/+e7dZHjXs5trb6NrephvJKTEd+kUAp/CKBUvhF\nAqXwiwRK4RcJlMIvEiiFXyRQGuffxxkrB/wx53a1n3Pbzj3Nn7J70fTJbv0wJEx9bdfy6cbW2OjW\nkwyasdOtX/vIotja+JH+z129eKn/4Albo6Mp3c/W1unILxIohV8kUAq/SKAUfpFAKfwigVL4RQKl\n8IsESuP8EW+sHPDHy9dMONJtu8tq3PpfzVzm1hOWC0g3Vp80Fp5w/kPTK/7S3peuOSu29s73/bUG\nPr/YLSf/n2mc36Ujv0igFH6RQCn8IoFS+EUCpfCLBErhFwmUwi8SKI3zR9KMlV86dr5bv2DhRLc+\n8OMlbr2qUye3bg0p1rdP2HQgaZvspl273PqaOfHnQNx91R1u2xt7j3brezducuvuOQpmftsAJB75\nSc4iuYXkirzrupN8kuSq6Gu30nZTRIqtkJf99wI4fb/rrgGw2MwGAlgcfS8irUhi+M3saQDb9rt6\nDIDZ0eXZAM4scr9EpMRa+p6/1sw2Rpc3AaiNuyHJCQAmAEBH+O9dRaR8Un/ab2YGIPbTEzObaWZ1\nZlZXgw5pH05EiqSl4d9MsjcARF+3FK9LIlIOLQ3/AgDjo8vjAfhjXSJScRLf85OcA+BUAD1Jrgdw\nA4CbAPyW5IUA1gI4p5SdLIak8eqk9e0/OG94bO3kTtPctgvu2P/z0k9LOsOgadfuhBuUbt562nX9\ne814Pra2/JJ+btvV3x/g1g+/wR/nT7OfQQgSw29mY2NKI4vcFxEpI53eKxIohV8kUAq/SKAUfpFA\nKfwigaKVcWrjIexuw9g6BwlOeiV+menfbz7Kbdt+Sle3vnlYZ7fecLBbRkMX5/8wYd3vg7b6S3N3\nWeffQecN/pTe6i07Ymtbp/mDTdcc9bhbv3vIsW696eOP44sJS5K31im/S2wxdti2hB8uR0d+kUAp\n/CKBUvhFAqXwiwRK4RcJlMIvEiiFXyRQbWbp7rRTdtdOPdGtP9FrRmytZ81Ot+27M7q49c17DnHr\n7+85yK3vaoz/2aur/HH6fgdtd+tHd/KnzX6h/Wa33qMqfqz9xV1HuG2/fXD8OQIAMHXiELd+2LTn\nYmtptmRvK3TkFwmUwi8SKIVfJFAKv0igFH6RQCn8IoFS+EUC1brm81c5Y7MJy1e3G+zPub/9sVlu\n/exfXB1b88aTAYAd/J2KbHfC0twZSjp/oqqrf44CO3aMre3d8I7bdtVtw9z6lNMecusPDOrr1l2t\ndL6/5vOLSCKFXyRQCr9IoBR+kUAp/CKBUvhFAqXwiwSqVc3nd7dcThjnf+NH/uL392w7ya0njeV7\nEsfxE8aUk+aep5G0BXfSvPbG9/ztxxPHyx3H/Ns6tz7oWxvd+rZ/+ofYWvdZ8VuHA2HM90888pOc\nRXILyRV5100huYHksujf6NJ2U0SKrZCX/fcCOL2Z66eZ2ZDo38LidktESi0x/Gb2NICE13Yi0tqk\n+cDvEpKvRm8LusXdiOQEkvUk6xtQueewi4SmpeGfAWAAgCEANgK4Je6GZjbTzOrMrK4G/gQXESmf\nFoXfzDabWaOZNQG4C8DQ4nZLREqtReEn2Tvv27MArIi7rYhUpsRxfpJzAJwKoCfJ9QBuAHAqySEA\nDMAaABOL0puEMWFr2BNbq+rs73F/84n/5davm3O+Wz8czhrwSfP198T3O3cDf254pmPKKcbpE++6\npr1bT5rvf+7zF7n1ugv/FFvb7i/fAGuqzPn6xZQYfjMb28zV95SgLyJSRjq9VyRQCr9IoBR+kUAp\n/CKBUvhFAlVZU3qZ8LfI4qef7hpxjNv0650WufW7fveB/9BerSFhKK5Cl3kuSAn7njSdOMkRM/xh\nyKn3PxJbm3TKxW7bqqf+6D+4t4w8kLiUfCXQkV8kUAq/SKAUfpFAKfwigVL4RQKl8IsESuEXCVRF\njfOzKmFKb1N87e1R/o/yxMeH+ff9x9fcuqsVjOlWpKTnLWEsveoP/lj8bVv+Jra2ZqJ//sKAp9xy\nqt/VSqEjv0igFH6RQCn8IoFS+EUCpfCLBErhFwmUwi8SqIoa50/jmyOWuvWpK/7OrfdBwji/N+as\ncf6SSDuW/sKdX4qtPf7jm922l/b9R7e+d/0G/8GTljyvgDUedOQXCZTCLxIohV8kUAq/SKAUfpFA\nKfwigVL4RQJVyBbd/QDcB6AWueXrZ5rZdJLdAcwF0B+5bbrPMbPtiY/ojJcnbUVdfXi/2NrkXnPc\ntk/d/2W/XwnYzum3xvlLIu26/j1mvRhbe+TyY922b1wZ/7sGAEde7o/zs7rGrXvbzZdLIUf+vQCu\nNLPBAIYDmERyMIBrACw2s4EAFkffi0grkRh+M9toZi9Hl3cCWAmgD4AxAGZHN5sN4MxSdVJEiu+A\n3vOT7A/gBABLANSa2caotAm5twUi0koUHH6SBwN4CMBlZrYjv2Zmhpjt7EhOIFlPsr4Bu1N1VkSK\np6Dwk6xBLvgPmNnD0dWbSfaO6r0BbGmurZnNNLM6M6urQYdi9FlEiiAx/CQJ4B4AK83s1rzSAgDj\no8vjAcwvfvdEpFQKmdL7FQDjACwnuSy67loANwH4LckLAawFcE7iPdGfppk0RfPdr/aNrXVJmP7Z\n5783ufWkQaW0w07SAgnTXlnt//p6Q8f33jnabfvAZbe79X+5dYxb37tuvVt3p/yWabpvYvjN7BkA\ncT0dWdzuiEi56Aw/kUAp/CKBUvhFAqXwiwRK4RcJlMIvEihaGZcQPqSquw2v/kZsPWlK75b5g2Jr\ndYetc9u+Pewjv3MJ20Free6w9H/xILf+7PoBbr3P2f5S8KxpH1tLM913iS3GDtuWsG54jo78IoFS\n+EUCpfCLBErhFwmUwi8SKIVfJFAKv0igyrtFt/lj+e0O7eo2n338vbG1cdOucNsehufcetrtoKUC\npdhWfe0PvuDW/2fenW7961dc5dZ73xr/++idAwAUb9lvHflFAqXwiwRK4RcJlMIvEiiFXyRQCr9I\noBR+kUCVd5w/wdazBrv1GjwWW+s7318n3V8pALCm8q1rIGXijOUnrvlfv8Ktf/OGH7r1RT+92a1/\n+//iz0vpNG+J29bte9Iveh4d+UUCpfCLBErhFwmUwi8SKIVfJFAKv0igFH6RQCWu20+yH4D7ANQC\nMAAzzWw6ySkALgKwNbrptWa20LuvQ9jdhjF+V+8dj/lzqHt1il97f/cpm9y2afZyl/CknVO/avpw\nt7707Ftja+eNHOe2bfzz6tjagazbX8hJPnsBXGlmL5PsAmApySej2jQz+/dCHkhEKkti+M1sI4CN\n0eWdJFcC6FPqjolIaR3Qe36S/QGcAGDf+YeXkHyV5CyS3WLaTCBZT7K+AbtTdVZEiqfg8JM8GMBD\nAC4zsx0AZgAYAGAIcq8MbmmunZnNNLM6M6urQYcidFlEiqGg8JOsQS74D5jZwwBgZpvNrNHMmgDc\nBWBo6bopIsWWGH6SBHAPgJVmdmve9b3zbnYWAH8alIhUlEI+7f8KgHEAlpNcFl13LYCxJIcgN/y3\nBsDEpDtq6tYZO78RPwQy96/9gYPvTrw8ttYB/lCfpuzKgUhcHjthS/eBk19w6986+vzY2nEPbHDb\nrv6yWy5YIZ/2PwOguXFDd0xfRCqbzvATCZTCLxIohV8kUAq/SKAUfpFAKfwigSrr0t29+mzHpKkP\nxta/9pC/HPKRjzljpwnjrklbMouU08HXd4qtXfjg027byX9/SWytaZF/fkE+HflFAqXwiwRK4RcJ\nlMIvEiiFXyRQCr9IoBR+kUAlLt1d1AcjtwJYm3dVTwDvlq0DB6ZS+1ap/QLUt5YqZt8ON7Nehdyw\nrOH/zIOT9WZWl1kHHJXat0rtF6C+tVRWfdPLfpFAKfwigco6/DMzfnxPpfatUvsFqG8tlUnfMn3P\nLyLZyfrILyIZUfhFApVJ+EmeTvJPJN8keU0WfYhDcg3J5SSXkazPuC+zSG4huSLvuu4knyS5Kvra\n7B6JGfVtCskN0XO3jOTojPrWj+TvSb5O8jWSk6PrM33unH5l8ryV/T0/yXYA/gzgbwGsB/ASgLFm\n9npZOxKD5BoAdWaW+QkhJL8K4EMA95nZsdF1Pwewzcxuiv5wdjOzH1VI36YA+DDrbduj3aR6528r\nD+BMABcgw+fO6dc5yOB5y+LIPxTAm2b2lpntAfAbAGMy6EfFM7OnAWzb7+oxAGZHl2cj98tTdjF9\nqwhmttHMXo4u7wSwb1v5TJ87p1+ZyCL8fQCsy/t+PTJ8ApphABaRXEpyQtadaUatmW2MLm8CUJtl\nZ5qRuG17Oe23rXzFPHct2e6+2PSB32eNMLMhAEYBmBS9vK1IlnvPVkljtQVt214uzWwr/xdZPnct\n3e6+2LII/wYA/fK+7xtdVxHMbEP0dQuAeai8rcc379shOfq6JeP+/EUlbdve3LbyqIDnrpK2u88i\n/C8BGEjyCJLtAXwHwIIM+vEZJDtHH8SAZGcAp6Hyth5fAGB8dHk8gPkZ9uVTKmXb9rht5ZHxc1dx\n292bWdn/ARiN3Cf+qwFcl0UfYvo1AMAr0b/Xsu4bgDnIvQxsQO6zkQsB9ACwGMAqAIsAdK+gvv0a\nwHIAryIXtN4Z9W0Eci/pXwWwLPo3OuvnzulXJs+bTu8VCZQ+8BMJlMIvEiiFXyRQCr9IoBR+kUAp\n/CKBUvhFAvX/2DpahBihxFsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c81e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# you need a matplotlib inline to be able to show images in python notebook\n",
    "%matplotlib inline\n",
    "plt.imshow(train_dataset[i])\n",
    "plt.title(\"Char \" + letter(train_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle Data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter a:52909\n",
      "letter b:52911\n",
      "letter c:52912\n",
      "letter d:52911\n",
      "letter e:52912\n",
      "letter f:52912\n",
      "letter g:52912\n",
      "letter h:52912\n",
      "letter i:52912\n",
      "letter j:52911\n"
     ]
    }
   ],
   "source": [
    "letters = set(train_labels)\n",
    "list_train_labels = train_labels.tolist()\n",
    "for l in letters:\n",
    "    print ('letter ' + letter(l) + ':' + str(list_train_labels.count(l)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune train_size as needed.\n",
    "Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (200000, 28, 28) (200000,)\n",
      "Validation (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "\n",
    "valid_dataset = train_dataset[:valid_size,:,:]\n",
    "valid_labels = train_labels[:valid_size]\n",
    "train_dataset = train_dataset[valid_size:valid_size+train_size,:,:]\n",
    "train_labels = train_labels[valid_size:valid_size+train_size]\n",
    "\n",
    "print('Training', train_dataset.shape, train_labels.shape)\n",
    "print('Validation', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 718193866\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "# load data\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (18724, 28, 28, 1) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Simple CNN with 2 convolutional layers\n",
    "\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)    \n",
    "    conv2 = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    \n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-73-888757f26811>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.997457\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 200: 1.384965\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 400: 0.679950\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 600: 0.986095\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 800: 0.752625\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1000: 0.685861\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 200 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 CNN with 2 convolutional layers and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    hidden = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    hidden2 = tf.nn.max_pool(hidden2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-76-888757f26811>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 3.708541\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 200: 1.070817\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 400: 0.754164\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 600: 0.862056\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 800: 0.631553\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.820346\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Test accuracy: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 200 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 CNN with 2 convolutional layers with pooling and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-77-008f508173b7>:73: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 6.200665\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 7.3%\n",
      "Minibatch loss at step 200: 2.249818\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 34.5%\n",
      "Minibatch loss at step 400: 2.143424\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 47.8%\n",
      "Minibatch loss at step 600: 2.014077\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 60.4%\n",
      "Minibatch loss at step 800: 1.705181\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 1000: 1.809105\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 71.0%\n",
      "Test accuracy: 77.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    hidden = max_pool_2x2(hidden)\n",
    "    \n",
    "    conv2 = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    hidden2 = max_pool_2x2(hidden2)\n",
    "    \n",
    "    shape = hidden2.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # add dropout\n",
    "    hidden3_dropout = tf.nn.dropout(hidden3, keep_prob)\n",
    "    \n",
    "    return tf.matmul(hidden3_dropout, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset,.5)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "  #global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  #learning_rate = tf.train.exponential_decay(0.1, global_step,5000,0.96,staircase=True)\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset,1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset,1.0))\n",
    "\n",
    "# Run model\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 200 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
